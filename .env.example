# API Configuration
# Get your OpenAI API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
# Get your Gemini API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
# Ollama host URL (default for local installation)
OLLAMA_HOST=http://localhost:11434

# Model Configuration
# OpenAI model to use (e.g., gpt-4o-mini, gpt-4o, gpt-3.5-turbo)
OPENAI_MODEL=gpt-4o-mini
# Gemini model to use (e.g., gemini-2.5-flash, gemini-1.5-pro)
GEMINI_MODEL=gemini-2.5-flash
# Ollama model to use (e.g., llama3.1, deepseek-r1, mistral, qwen3)
OLLAMA_MODEL=llama3.1
# Temperature for LLM responses (0.0-1.0, higher = more creative)
LLM_TEMPERATURE=0.7
# Language for LLM responses (e.g., English, Spanish, French, German, etc.)
LLM_LANGUAGE=English
# Default LLM to use (openai, gemini, or ollama)
DEFAULT_LLM_MODEL=gemini

# Search Configuration
# Web search engine function (perplexica_search or brave_search_function)
WEB_RETRIEVAL_ENGINE_FUNCTION=perplexica_search
# Perplexica API endpoint (if using Perplexica)
PERPLEXICA_API_URL=http://localhost:3000/api/search
PERPLEXICA_CHAT_PROVIDER=gemini
PERPLEXICA_CHAT_NAME=gemini-2.0-flash
PERPLEXICA_EMBEDDING_PROVIDER=gemini
PERPLEXICA_EMBEDDING_NAME=models/text-embedding-004
PERPLEXICA_OPTIMIZATION_MODE=speed
PERPLEXICA_STREAM=false
PERPLEXICA_HISTORY=[]

# Debug and Logging
# Set to "true" to enable printing of raw LLM responses
DEBUG_RAW_MESSAGES=false
# Directory for log files
LOGS_DIR=logs
# Maximum words to keep in context window
MAX_LAST_CONTEXT_WORDS=512

# Safety Configuration
# Additional safe commands that don't require confirmation (comma-separated)
# Default safe commands: date,pwd,whoami,uname,ls,cat,head,tail,grep,wc,echo,which,id,uptime,df,free,ps
# Example: SAFE_COMMANDS=hostname,history,find,sort,uniq,awk,sed
SAFE_COMMANDS=

# Security Settings
# Enable strict safe mode for enhanced security warnings
SAFE_MODE_STRICT=true

# Maximum command length allowed (helps prevent injection attacks)
MAX_COMMAND_LENGTH=500

# Command execution timeout in seconds (0 = no timeout)
COMMAND_TIMEOUT=60

# Token Tracking
# Initial token count (will be updated during runtime)
TOTAL_TOKENS_USED=0
